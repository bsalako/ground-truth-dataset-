# Ground-truth-dataset for GRC & AI Governance
A structured, framework‑aligned ground truth dataset designed to evaluate AI systems on Governance, Risk & Compliance (GRC) and AI Governance tasks.
This repository includes authoritative answers, evidence requirements, framework mappings, risk classifications, and acceptable variations for 25+ evaluation questions.

This project demonstrates practical, hands‑on AI governance work, including dataset design, evaluation methodology, and governance documentation.

Purpose of This Repository

Modern AI systems require high‑quality ground truth datasets to evaluate:

    Accuracy

    Completeness

    Compliance alignment

    Safety

    Evidence reasoning

    Framework mapping

    Risk awareness

This repository provides a structured dataset and supporting documentation that reflect how real GRC and AI governance teams evaluate AI outputs.

It is designed as both a portfolio artifact and a practical evaluation resource.
